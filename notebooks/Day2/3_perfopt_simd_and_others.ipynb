{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099128b6-1688-4176-bd8b-93c6aff80cb5",
   "metadata": {},
   "source": [
    "# Optimizing Performance (Single-Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bea96e-1cd8-471f-a577-4b3e5cbf78d1",
   "metadata": {},
   "source": [
    "## SIMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d9816-7820-41fb-bf51-71bbe88526aa",
   "metadata": {},
   "source": [
    "SIMD stands for **\"Single Instruction Multiple Data\"** and falls into the category of instruction level **parallelism** (vector instructions). Since raw clock speeds haven't been getting much faster, one way in which processors have been able to increase performance is through operations which operate on a \"vector\" (basically, a short sequence of values contiguous in memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774884a6-0243-4677-b2cd-a7bfc737f808",
   "metadata": {},
   "source": [
    "Consider this simple vector addition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b5438-6556-4ccf-a9c5-c1c83b1a4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_add(A, B, C)    \n",
    "    for i in eachindex(A, B, C)\n",
    "        @inbounds A[i] = B[i] + C[i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fff18e-c614-4162-8594-871653240929",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "*Packed* vector addition: **vaddpd**\n",
    "\n",
    "<img src=\"./imgs/SIMD.svg\" width=450px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331f6b6-360b-40d7-9055-4b75b5ac3d12",
   "metadata": {},
   "source": [
    "\n",
    "The idea behind SIMD is to perform the add instruction on multiple elements at the same time (instead of separately performing them one after another). The process of splitting up the simple loop addition into multiple vector additions is often denoted as \"loop vectorization\". Since each vectorized addition happens at instruction level, i.e. within a CPU core, the feature set of the CPU determines how many elements we can process in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8f7c0-cb70-413e-8446-e3f95d0dd622",
   "metadata": {},
   "source": [
    "#### SIMD register width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbe0d5-5362-4d39-9f72-aef3a38eb847",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"./imgs/SIMD_vectorwidth.svg\" width=580px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d566cb-d6de-45da-90cc-a0668fb218cb",
   "metadata": {},
   "source": [
    "### Is SIMD important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab84c30-142e-4aff-accb-cb997867f3ca",
   "metadata": {},
   "source": [
    "**Peak performance** (single-core): $P_\\textrm{core} = f \\cdot n_\\textrm{super} \\cdot n_\\textrm{FMA} \\cdot n_\\textrm{SIMD}$\n",
    "- $f$: clock frequency\n",
    "- $n_\\textrm{super}$: superscalarity (multiple arithmetic units)\n",
    "- $n_\\textrm{FMA}$: FMA factor (two FLOPs in one instruction)\n",
    "- $n_\\textrm{SIMD}$: SIMD factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e315d94-a417-496d-bb1b-d47c3a6bb9f1",
   "metadata": {},
   "source": [
    "|microarchitecture|processor|launch date|f [GHz]|n_super|n_FMA|n_SIMD|P_core [GFLOPS]|\n",
    "|:----|:----|:----|:----|:----|:----|:----|:----|\n",
    "|Haswell|Xeon E5-2695 v3|Q3/2014|2.30|2|2|4|36.8|\n",
    "|Skylake SP|Xeon Gold 6148|Q3/2017|2.40|2|2|8|76.8|\n",
    "|Zen 2|EPYC 7642|Q3/2019|2.30|2|2|4|36.8|\n",
    "|Zen 3|EPYC 7763|Q1/2021|2.45|2|2|4|39.2|\n",
    "|A64FX|FX1000|Q1/2020|2.20|2|2|8|70.4|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae10e2b-4317-41c0-b784-f52305afab30",
   "metadata": {},
   "source": [
    "### What does my system support?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695a382-395c-4b68-8d56-6ba273a070f0",
   "metadata": {},
   "source": [
    "Let's check which \"advanced vector extensions\" (AVX) the system supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36144590-342f-4c78-8b3e-cfe048482457",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CpuId\n",
    "cpuinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa02001-f74f-472b-ad89-4125c0a0e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(x -> contains(string(x), \"AVX\"), cpufeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d754be-18fe-42d0-b374-d4f66503decf",
   "metadata": {},
   "source": [
    "**Noctua 2 nodes do not have AVX512.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43dedb-fbec-49d6-9b04-7908cafc3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 512^2\n",
    "A = rand(Float64, SIZE);\n",
    "B = rand(Float64, SIZE);\n",
    "C = rand(Float64, SIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81133702-a8c4-4e32-a161-ec253284002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_add(A,B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3028be3-f7ca-4fd4-86d6-c09fecb12590",
   "metadata": {},
   "source": [
    "### It's not always so simple: SIMD can be hard..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69249dcc-96f4-4999-b9a8-ea7eff7d74ae",
   "metadata": {},
   "source": [
    "Autovectorization is a hard problem (it needs to prove a lot of things about the code!). After all, it is a from of parallelism and efficient parallelism can be hard as well...\n",
    "\n",
    "Not every loop is (readily) vectorizable. **Keep your loops as simple as possible!**\n",
    "\n",
    "* avoid conditionals and function calls etc.\n",
    "* ideally, loop length is static (countable up front).\n",
    "* access **contiguous data** (spatial locality).\n",
    "  * (align data structures to SIMD width boundary)\n",
    "* avoid data dependencies (e.g. between loop iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f318b0c-b03b-4922-8c6d-19a8d0771611",
   "metadata": {},
   "source": [
    "#### Example: Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c54422-6992-4e35-9093-109a7cee99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ce616-66cb-41f9-892b-2aef805772d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af024b-ae44-4dbf-ba14-ec8a07e1662a",
   "metadata": {},
   "source": [
    "Note the `vaddsd` instruction and usage of `xmmi` registers (128 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c085bb0-ce9a-4e16-8c8a-ef03b62fe528",
   "metadata": {},
   "source": [
    "#### How could this loop reduction be vectorized manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4894c0-c181-4abb-91f3-1637cc0ba240",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_unrolled4(B, C)\n",
    "    a1 = zero(eltype(B))\n",
    "    a2 = zero(eltype(B))\n",
    "    a3 = zero(eltype(B))\n",
    "    a4 = zero(eltype(B))\n",
    "    @inbounds for i in 1:4:length(B)-4\n",
    "        a1 += B[i] * C[i]\n",
    "        a2 += B[i+1] * C[i+1]\n",
    "        a3 += B[i+2] * C[i+2]\n",
    "        a4 += B[i+3] * C[i+3]\n",
    "    end\n",
    "    return a1+a2+a3+a4\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ef00c-5eb4-4e01-bd3d-cf24e36d277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_unrolled4(B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31aa21a-6d99-45d0-bbfb-defdd421bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime vector_dot($B, $C) samples=10 evals=3;\n",
    "@btime vector_dot_unrolled4($B, $C) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c75c6-cc0f-4b4a-95a1-12815045b6d6",
   "metadata": {},
   "source": [
    "#### The \"automatic\" way: `@simd`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4b544-4e56-4511-a832-56c292b48b53",
   "metadata": {},
   "source": [
    "To (try to) \"force\" automatic SIMD vectorization in Julia, you can use the `@simd` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f20190-a80c-47b1-a09d-aad1d6d8bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_simd(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    @simd for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abd400-0e96-4234-a3ea-27b95a5a56d5",
   "metadata": {},
   "source": [
    "By using the `@simd` macro, we are **asserting several properties** of the loop:\n",
    "\n",
    "* It is safe to execute iterations in arbitrary or overlapping order, with special consideration for reduction variables.\n",
    "* Floating-point operations on reduction variables can be reordered, possibly causing different results than without `@simd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de9329-37d4-497a-ae7d-0c188084df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot_simd($B, $C) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4dea3a-4893-4a09-b402-1ae254efbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_simd(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838860a3-1683-4510-93be-5347f58d5707",
   "metadata": {},
   "source": [
    "Note the `vfmadd231pd` instruction and usage of `ymmi` AVX registers (256 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102a494-4e3c-4e3f-8251-4432b8e6f296",
   "metadata": {},
   "source": [
    "#### Data types matter\n",
    "Floating-point addition is **non-associative** and the order of operations is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67833b42-978b-4c35-9e4f-90a5373026ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = rand(10^6)\n",
    "@show vector_dot(v,v);\n",
    "@show vector_dot_simd(v,v);\n",
    "@show abs(vector_dot(v,v) - vector_dot_simd(v,v));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51c6ad-d539-4228-a98a-6ed53adff22f",
   "metadata": {},
   "source": [
    "How bad can this get? In principle, [arbitraily bad](https://discourse.julialang.org/t/when-shouldnt-we-use-simd/18276/11?u=carstenbauer)!! Quite often you can get away with it though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a57eb9-948d-4cb4-8c58-0333284b16e6",
   "metadata": {},
   "source": [
    "\n",
    "Compare this to integer addition, which is **associative** and the order of operations has no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8698a-b7dc-4355-aeee-a2967b57b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_int = rand(Int64, SIZE);\n",
    "C_int = rand(Int64, SIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa5924-8094-47d0-874f-47b0fc201f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show vector_dot(B_int, C_int);\n",
    "@show vector_dot_simd(B_int, C_int);\n",
    "@show abs(vector_dot(B_int, C_int) - vector_dot_simd(B_int, C_int));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54286dd-19b9-4a38-914c-cc49c4c173a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot($B_int, $C_int) samples=10 evals=3;;\n",
    "@btime vector_dot_simd($B_int, $C_int) samples=10 evals=3;;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d12f87-c969-420a-b751-e46ab228c78a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Data layout matters (Structure of Array vs Array of Structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49ba99-6b40-4d0c-9858-71dea46191d7",
   "metadata": {},
   "source": [
    "Contiguous memory access facilitates SIMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cab338-b00f-44af-abe7-5c602bb904d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complex_numbers_aos = [rand() + im * rand() for i in 1:1024] # array of structs (Complex{Float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008edb6-c794-44ef-a72e-b82fc41e456d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Base: sum\n",
    "\n",
    "struct ComplexNumbers\n",
    "    x::Vector{Float64}\n",
    "    y::Vector{Float64}\n",
    "end\n",
    "\n",
    "sum(cn::ComplexNumbers) = sum(cn.x) + im * sum(cn.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41f7bb-c5e7-4f9e-bf7e-4f2e1e08a2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complex_numbers_soa = ComplexNumbers(rand(1024), rand(1024)) # struct of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3935e83-0c6d-4ebd-9c43-8fb2e47622ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum($complex_numbers_aos);\n",
    "@btime sum($complex_numbers_soa);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3608861-3b4c-4f5f-9882-e0e2b6bc6889",
   "metadata": {},
   "source": [
    "Sidenote: [StructArrays.jl](https://github.com/JuliaArrays/StructArrays.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53d0f5-22f3-4712-b1c1-bf42c898bc09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other tricks: `@fastmath` (if time permits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe21e4-355a-4202-827d-6eda570e69a5",
   "metadata": {},
   "source": [
    "Enables lots of floating point optimizations that are potentially *unsafe*! It trades accuracy for speed, so, [Beware of fast-math](https://simonbyrne.github.io/notes/fastmath/). (See the [LLVM Language Reference Manual](https://llvm.org/docs/LangRef.html#fast-math-flags) for more information on which compiler options it sets.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f74d1-a662-40f4-af35-5a3a418ece3a",
   "metadata": {},
   "source": [
    "### SIMD\n",
    "Among other things, it **facilitates SIMD vectorization** because it:\n",
    "* Allows re-association of operands in series of floating-point operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700780fc-546c-4f42-b5bd-8f17ec4455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function vector_dot_fastmath(B, C)\n",
    "    a = zero(eltype(B))\n",
    "    @fastmath for i in eachindex(B,C)\n",
    "        @inbounds a += B[i] * C[i]\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45805f00-f097-4e86-8d18-ddcaf9c15d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime vector_dot_fastmath($B, $C) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d0bc4-e748-4ad5-82e8-25b9f7bac606",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none syntax=:intel vector_dot_fastmath(B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928d2f3-7672-4885-95be-862054c7ea77",
   "metadata": {},
   "source": [
    "### FMA - Fused Multiply Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d803f-ba30-4182-bd02-6ab801260668",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b,c) = a*b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1838aa-2cce-4745-9fc3-a1d0a394427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f(1.0,2.0,3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e29b4-ebd5-406a-b6c7-48b8e2adde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fastmath(a,b,c) = @fastmath a*b+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e173923-1d4e-41b1-8b64-aed48b4183b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f_fastmath(1.0,2.0,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fc76b-3016-47d8-a7ad-a32d17b6fb0b",
   "metadata": {},
   "source": [
    "(In this specific case, the explicit `fma` function or [MuladdMacro.jl](https://github.com/SciML/MuladdMacro.jl) are *safer* alternatives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48ef2a-8a79-447b-8530-f72751d2f980",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/skylake_server_microarch.png\" width=900px>\n",
    "\n",
    "**Source:** [Intel® 64 and IA-32 Architectures Optimization Reference Manual](https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bb834-f4db-40ce-991a-f5deed458662",
   "metadata": {},
   "source": [
    "#### Sidenote: Why doesn't Julia use FMA automatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46fc61-7118-4298-8e06-68ed0911b7ef",
   "metadata": {},
   "source": [
    "Answer: because it can break math in weird ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5725fc-2c62-4de8-8ecd-e7b888769e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function f(a,b,c)\n",
    "    @assert a*b ≥ c\n",
    "    return sqrt(a*b-c)\n",
    "end\n",
    "\n",
    "function f_fma(a,b,c)\n",
    "    @assert a*b ≥ c\n",
    "    return sqrt(fma(a,b,-c))\n",
    "end\n",
    "\n",
    "a = 1.0 + 0.5^27;\n",
    "b = 1.0 - 0.5^27;\n",
    "c = 1.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89393aa5-6f9a-434b-9bc2-585270c3cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462b7f1-9b87-4c29-a2bd-6e8715334f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fma(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a9303-100c-4f16-b0c1-2203f07c4c1d",
   "metadata": {},
   "source": [
    "# Core messages of this Notebook\n",
    "\n",
    "* **SIMD is important for your innermost computational kernel** and, ideally, can give you a factor of 4 or 8 speedup (for `Float64`).\n",
    "* **Keep your hot loop as simple as possible** to facilitate SIMD (avoid branches, data dependencies, etc., if possible).\n",
    "* (Carefully) think about using `@simd`, `@fastmath`, etc. to **opt-into potentially unsafe optimizations**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (1 thread) 1.10.0",
   "language": "julia",
   "name": "julia-_1-thread_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
