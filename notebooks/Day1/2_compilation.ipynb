{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3d18f",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "To be fast, Julia needs to **specialize** code, that is **compile specific native versions of the code** utilizing the type information. **The better the specialization the faster the code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add0fc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## \"Just ahead of time\" compilation\n",
    "\n",
    "* Julia **specializes on the types of function arguments** and \n",
    "* compiles efficient machine code **when a function is called for the first time** (with these input argument types).\n",
    "\n",
    "If the same function is called again with the same input argument types, the already existing machine code is reused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2352e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "func(x,y) = 2x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.2, 3.4, 5.6] # Vector{Float64}\n",
    "y = [0.4, 0.7, 0.9] # Vector{Float64}\n",
    "\n",
    "@time func(x,y);\n",
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a475ea",
   "metadata": {},
   "source": [
    "**First call:** compilation + running the code\n",
    "\n",
    "**Second call:** running the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44115394",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74b788e",
   "metadata": {},
   "source": [
    "If one of the input types changes, Julia compiles a new specialization of the function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb28c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49854ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time func(x,y); # Vector{Int64}, Vector{Float64}\n",
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9af6e-a348-4871-aa89-53f310098baa",
   "metadata": {},
   "source": [
    "We now have two efficient native codes in the cache: one for all `Vector{Float64}` inputs and another one for `Vector{Int64}` as the first and `Vector{Float64}` as the second argument type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8c246-cad7-4155-8fc5-899862be6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8254a81-c2a3-4773-9c4f-7ed7ef160dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MethodAnalysis\n",
    "methodinstances(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d5e0dc-d5ea-4e8c-b285-df29f3a37105",
   "metadata": {},
   "source": [
    "### Compilation pipeline\n",
    "\n",
    "<p><br><img src=\"imgs/Julia_compilation_pipeline.svg\" width=\"512\"/></p>\n",
    "\n",
    "* **AST**: abstract syntax tree\n",
    "* **IR**: intermediate representation\n",
    "\n",
    "More about Julia compilation, see [Bezanson J et al (2018) Julia: dynamism and performance reconciled by design. Proc ACM Program Lang.](https://doi.org/10.1145/3276490)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4fcc5-1839-4d3b-9243-886e3be7bd4e",
   "metadata": {},
   "source": [
    "### What makes Julia fast?\n",
    "\n",
    "**Specialization** → (Successful) **Type inference** → **Compilation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6c894-0b8b-4457-aa21-c82389fa7671",
   "metadata": {},
   "source": [
    "## Introspection tools\n",
    "#### (*But I really want to see what happens!*)\n",
    "\n",
    "We can inspect the code at all transformation stages with a bunch of macros:\n",
    "\n",
    "<img src=\"./imgs/julia_introspection_macros.svg\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@macroexpand @show 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33406d-9c41-496b-abce-552cdd221c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x, y) = x^3 + y/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c3d5-fa28-41f6-a6a8-2d8eb664894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_lowered f(1.0,2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed f(1.0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7543e88-4a3a-48a2-92be-9832b8a69d47",
   "metadata": {},
   "source": [
    "From the types of the input arguments, Julia has figured out all the intermediate types. This crucial process is known as **type inference** and its success is the basis for a good specialization (i.e. performant native code as a result). Moreover, the generic power function computing the cubic of `x` is replaced by specific floating-point multiplications (**static dispatch**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a33e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f(1.0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1afc8-f12d-4922-95ea-9e7e15a64040",
   "metadata": {},
   "source": [
    "The expensive divide operation (`y/2`) is replaced by multiplying by 0.5. In the end, giving two `Float64` arguments this function has 4 floating-point operations, i.e. 3 multiplications and 1 addition, instead of cubic function and division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae32331",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f(1.0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6ed56",
   "metadata": {},
   "source": [
    "Let's compare this to integer inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none f(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2ea6d",
   "metadata": {},
   "source": [
    "### Recommendation: [Cthulhu.jl](https://github.com/JuliaDebug/Cthulhu.jl)\n",
    "While these introspection macros are great, we recommend to use `@descend` from the package [Cthulhu.jl](https://github.com/JuliaDebug/Cthulhu.jl) for real world code analysis.\n",
    "\n",
    "Essentially, Cthulhu is an **interactive**, more powerful generalization of the macros above.\n",
    "\n",
    "* Allows easy switching between code representations (syntax, typed, native, ...).\n",
    "* **Recursive application possible**(!) (i.e. introspecting a function that is called within a function within function ...).\n",
    "\n",
    "However, due to its interactivity, it doesn't work in Jupyter but **only works in the REPL** (→ exercise).\n",
    "\n",
    "<img src=\"./imgs/cthulhu.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45872508",
   "metadata": {},
   "source": [
    "## How important is specialization?\n",
    "\n",
    "Let's try to estimate the performance gain by specialization.\n",
    "\n",
    "To prevent specialization, we deliberately throw away any useful type information and operate on a `Vector{Any}` that can literally store anything!\n",
    "\n",
    "(This is qualitatively comparable to what Python does.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func(v) = 2*v[1] + v[2] # version of func that takes in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3134a-027f-41c3-8d7f-11ffe11ca355",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6ae38-70dd-42a3-a723-8b4eea2bacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Any[rand(), rand()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49529fb",
   "metadata": {},
   "source": [
    "For benchmarking we will use `@btime` (or `@benchmark`) from [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl). This will take care of a couple of things for us:\n",
    "* Exclude first run.\n",
    "* Run the code multiple times (→ statistics).\n",
    "* Benchmark in a function (local scope).\n",
    "\n",
    "**General rule:** For proper benchmarking don't use `@time` but `@btime` and interpolate (`$`) global input arguments.\n",
    "\n",
    "(Prefixing variable with `$` always means interpolation in Julia, e.g. string interpolation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71626b0-6334-4c0b-aee8-c6904d1c3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ucl = \"UCL ARC\"\n",
    "welcome = \"Welcome to $ucl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eadf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "v_typed = rand(2)\n",
    "v_any = Any[rand(), rand()]\n",
    "\n",
    "@btime func($v_typed);\n",
    "@btime func($v_any);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark func($v_any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf2ba3-ac9b-4cdf-a5a0-17e748f1b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed func(rand(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4ac87-01c7-4a4f-9e78-64a298bbdec7",
   "metadata": {},
   "source": [
    "**static dispatch**: the generic functions `*` and `+` are replaced by specific implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cd881-b2f0-4559-8015-5953c3fea257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed func(Any[rand(), rand()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21663ee4-0c69-4d76-83de-cea6a56a791f",
   "metadata": {},
   "source": [
    "Note here the generic functions `*` and `+` can not be replaced by specific variants due to lack of type information. This leads to inefficient **runtime dispatch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90c012",
   "metadata": {},
   "source": [
    "## Dispatch and specialization\n",
    "\n",
    "**Types drive both dispatch and specialization.**\n",
    "\n",
    "First, the most specific method is selected (dispatch), then it gets compiled to efficient native code (specialization). Let's reconsider our earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "myabs(x::Real) = sign(x) * x\n",
    "myabs(z::Complex) = sqrt(real(z * conj(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9145b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native myabs(3.2 + 4.5im) # complex input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fb226",
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native myabs(3 + 4im) # also complex input but different native code (due to specialization)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dbe57",
   "metadata": {},
   "source": [
    "## Are explicit type annotations necessary? (think C or Fortran)\n",
    "\n",
    "Note that Julia's type inference is powerful. Specifying types **is not** necessary for best performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_function(x)\n",
    "    y = rand()\n",
    "    z = rand()\n",
    "    x+y+z\n",
    "end\n",
    "\n",
    "function my_function_typed(x::Int64)::Float64\n",
    "    y::Float64 = rand()\n",
    "    z::Float64 = rand()\n",
    "    x+y+z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime my_function(10);\n",
    "@btime my_function_typed(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd19c86",
   "metadata": {},
   "source": [
    "Annotating types explicitly can serve a purpose.\n",
    "\n",
    "* Enforce conversions\n",
    "* Rather rarely: help the compiler infer types in tricky situations\n",
    "\n",
    "However, more often than not it is an indication of suboptimal code design. (It also makes functions much less generic and reusable!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64848b-6e37-4cf8-a08d-29df33a930e3",
   "metadata": {},
   "source": [
    "## Compilation on heterogeneous HPC clusters\n",
    "\n",
    "By default, Julia produces native code for the CPU type it is running on. This means that it uses the [Instruction Set Architecture (ISA)](https://en.wikipedia.org/wiki/Instruction_set_architecture) of this CPU.\n",
    "\n",
    "This can lead to issues on heterogeneous clusters where different nodes have different CPU types. E.g. you precompile Julia packages on a login node with an Intel CPU but want to run the code on a compute node with AMD CPUs.\n",
    "\n",
    "**Solution: Multiversioning**\n",
    "\n",
    "```julia\n",
    "# Noctua 1 & 2\n",
    "export JULIA_CPU_TARGET=\"generic;znver3,clone_all;skylake,clone_all\"\n",
    "\n",
    "# Noctua 2 & DGX\n",
    "export JULIA_CPU_TARGET=\"generic;znver3,clone_all;znver2,clone_all\"\n",
    "```\n",
    "\n",
    "This will compile a generic (but slow) variant as well as efficient variants for AMD Zen3 and Intel Skylake CPUs / AMD Zen2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271288b7",
   "metadata": {},
   "source": [
    "# Core messages of this Notebook\n",
    "\n",
    "* **A function is compiled when called for the first time** with a given set of argument types.\n",
    "* There are **multiple code transformation steps** which can be inspected through macros like `@code_warntype` or `@descend` from Cthulhu.jl.\n",
    "* What makes Julia fast? Successful **Type inference** → **Specialization** → **Compilation**.\n",
    "* Functions should almost always be benchmarked with **BenchmarkTools.jl's `@btime` and `@benchmark`** instead of `@time`.\n",
    "* In virtually all cases, **explicit type annotations are irrelevant for performance**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (1 thread) 1.10.0",
   "language": "julia",
   "name": "julia-_1-thread_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
