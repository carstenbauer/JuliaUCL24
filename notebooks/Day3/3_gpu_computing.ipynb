{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"./imgs/Julia-code-cpu-gpu.png\" width=768>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA A100 GPU\n",
    "\n",
    "<img src=\"./imgs/a100_front.png\" width=512px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming Multiprocessor in NVIDIA A100**\n",
    "\n",
    "<img src=\"./imgs/a100_SM.png\" width=512px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "| Compute units              | Count            |\n",
    "|----------------------------|------------------|\n",
    "| **SMs**                    | 108              |\n",
    "| **CUDA cores** / FP32 ALUs | 6912 (64 per SM) |\n",
    "| **Tensor cores**           | 432 (4 per SM)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/gpu_topology.svg\" width=500px>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory)\n",
    "* **SM**: Streaming Multiprocessor\n",
    "\n",
    "Communication:\n",
    "* Host-device bandwidth: **31.5 GB/s**\n",
    "* GPU global memory bandwidth: **1555 GB/s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick comparison: CPU vs GPU\n",
    "\n",
    "### AMD EPYC 7763 vs NVIDIA A100\n",
    "\n",
    "|               | compute units   | maximum clock frequency [GHz] | FP32 peak performance [TFLOPS] |\n",
    "|:-------------:|:---------------:|:-----------------------------:|:------------------------------:|\n",
    "| AMD EPYC 7763 |  64 x86 cores   |  3.50                         |  5.0                           |\n",
    "| NVIDIA A100   | 6912 CUDA cores |  1.41                         | 19.5 (155.9 for Tensor cores)                     |\n",
    "\n",
    "Most of the [top500](https://top500.org/lists/top500/2023/11/) systems have GPUs as accelerators and they are dominating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./imgs/cpu_gpu_fraction.svg\" width=768>\n",
    "\n",
    "**Source:** [J. Apostolakis et al., *Detector simulation challenges for future accelerator experiments.*, Frontiers in Physics 10 (2022)](https://doi.org/10.3389/fphy.2022.913510)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between CPU and GPU\n",
    "\n",
    "|                   | CPU                               | GPU                                 |\n",
    "|:-----------------:|:---------------------------------:|:-----------------------------------:|\n",
    "| optimized for     | latency and per-core performance  | computational throughput            |\n",
    "| cores             | complex                           | rather simple                       |\n",
    "| number of threads | O(100)                            | O(1_000_000)                        |\n",
    "| thread pinning    | a must for good performance       | not needed at all                   |\n",
    "\n",
    "### Memory-bound scientific computing\n",
    "\n",
    "The performance of most scientific codes is **memory-bound** (memory access speed) rather than compute-bound (how fast computations can be done). In a certain time interval, GPUs (and CPUs) can perform more computations than read numbers from memory.\n",
    "\n",
    "**Peak performance over peak memory bandwidth** (for NVIDIA A100 40GB SXM)\n",
    "\n",
    "$$\n",
    "\\dfrac{19.5 \\ [\\textrm{TFLOPS}]}{1.56 \\ [\\textrm{TB/s}]} \\cdot 4 \\ \\textrm{B} = 50\\ \\textrm{FLOPs}\n",
    "$$\n",
    "\n",
    "To achieve the peak FP32 performance one needs 50 FLOPs per FP32 number (i.e. `Float32`) from memory.\n",
    "\n",
    "Crucially, the peak memory bandwidth of GPUs is much higher than for CPUs: **~1.56 TB/s** (A100 40GB SXM) vs **~400 GB/s** (2x AMD EPYC 7763).\n",
    "\n",
    "(‚Üí exercise **saxpy_gpu** and **daxpy_cpu**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia + GPU (NVIDIA)\n",
    "\n",
    "Website: https://juliagpu.org/\n",
    "\n",
    "We'll focus on **NVIDIA GPUs** but there is [support for other GPUs](https://juliagpu.org/) (AMD, Intel, etc.) as well.\n",
    "\n",
    "The interface to NVIDIA GPU computing is the [CUDA language extension](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html). In Julia there is [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl).\n",
    "\n",
    "It leverages LLVM, specifically parts of the Julia compiler as well as [GPUCompiler.jl](https://github.com/JuliaGPU/GPUCompiler.jl), to compile **native GPU code**. (compare to `nvcc`)\n",
    "\n",
    "It provides:\n",
    "\n",
    "* **High-level abstraction `CuArray`**\n",
    "* **Tools for writing CUDA kernels**\n",
    "* **Wrappers to proprietary NVIDIA libraries (e.g. cuBLAS, cuFFT, cuSOLVER, cuSPARSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting CUDA\n",
    "\n",
    "By default, **it's automatic**. The CUDA toolkit is installed automatically when **using** CUDA.jl for the first time. The only requirement is a working NVIDIA driver.\n",
    "\n",
    "**Note:** You can readily add CUDA.jl to a Julia environment on a machine without GPUs, say, a login node. See [Precompiling CUDA.jl without CUDA](https://cuda.juliagpu.org/stable/installation/overview/#Precompiling-CUDA.jl-without-CUDA) for more information.\n",
    "\n",
    "#### System CUDA\n",
    "\n",
    "You can opt-out of the automatic system by setting a Julia preference, e.g.\n",
    "\n",
    "```julia\n",
    "CUDA.set_runtime_version!(v\"12.2\"; local_toolkit=true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.functional() # if this works, you're good to go üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device() # the currently selected GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array programming: `CuArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use a GPU is via **vectorized array operations** (e.g. broadcasting). Each of these operations will be backed by one or more GPU kernels, either natively written in Julia or from some application library. As long as your data is large enough you should be able to get nice speedups in many cases.\n",
    "\n",
    "You use the `CuArray` type, which serves a dual purpose:\n",
    "\n",
    "* a managed container for GPU memory\n",
    "* a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CuArray` is a **CPU object representing GPU memory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.rand(4) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"./imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cpu = [1,2,3,4]\n",
    "x_gpu = CuArray(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` or `copy!` to move it into already allocated memory)\n",
    "\n",
    "For better performance the data movement between CPU and GPU should be minimized as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array computations on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it, that we'd also do with regular `Array`s. (**duck typing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 2048\n",
    "A_gpu = CUDA.rand(N,N);\n",
    "B_gpu = CUDA.rand(N,N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu * B_gpu # we need CUDA.@sync because GPU operations are typically asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime CUDA.@sync(A_cpu * B_cpu) setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime CUDA.@sync(A_gpu * B_gpu) setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: `*` for `CuArray`s uses a cuBLAS kernel under the hood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More examples: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu .+ B_gpu; # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync sqrt.(A_gpu.^2 + B_gpu.^2); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync mapreduce(sin, +, A_gpu); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power of simple GPU array programming can not be underestimated!** Entire codes (like deep learning frameworks etc.) could be ported to GPU without ever writing a single CUDA kernel manually.\n",
    "\n",
    "Of course, it isn't always as easy or performance can be improved by writing custom kernels. (‚Üí exercise **heat_diffusion**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**:\n",
    "\n",
    "* The access to an individual element in an array, e.g. scalar indexing, must not be used in array programming.\n",
    "* You must express arithmetic operations in terms of arrays and treat the `CuArray` array as a whole entity, e.g.\n",
    "\n",
    "```julia\n",
    "function gpu_broadcasting!(C, A, B)\n",
    "    CUDA.@sync C .= A .* B\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few words on memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. If they are unreachable, they will get cleaned up automatically during a GC run. However, keep in mind that the (CPU-focused) GC isn't good at sensing GPU memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizeof(x_gpu) |> Base.format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default CUDA.jl uses a **memory pool** to speed up future allocations. So it might appear as if the objects have not been freed. (On Noctua 2 we have disabled it. You can disable the memory pool with `JULIA_CUDA_MEMORY_POOL=none`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` and `CUDA.reclaim()` to more aggressively suggest to release the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one must be careful with `CUDA.unsafe_free!` because one still has the handle `x_gpu` that now points to free'd memory. But it is fine and very useful in a pattern like this:\n",
    "\n",
    "```julia\n",
    "function myfunction(x::CuArray)\n",
    "    tmp_memory = similar(x)\n",
    "    expensive_operation!(x, tmp_memory)\n",
    "    CUDA.unsafe_free!(tmp_memory)\n",
    "    return x\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing # to be safe :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming: Writing CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level array programming is most suitable for some types of computations. It may be less efficient, or even not possible for general applications.\n",
    "\n",
    "**CUDA kernel**: a function that will be executed by all *GPU threads* in parallel.\n",
    "\n",
    "Based on the index of a thread we can make them operate on different pieces of given data.\n",
    "\n",
    "(It might be helpful to think of the CUDA kernel as being the body of a loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x # the thread index (\"loop index\")\n",
    "    x[i] += 1\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can launch the kernel on the GPU with the `@cuda` macro (non-blocking, asynchronous):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imaging, kernel programming can become more difficult, especially if you care about performance. A few reasons:\n",
    "\n",
    "* you need to respect **hardware limitations** of the GPU, e.g. maximum number of GPU threads per block\n",
    "* **not all operations can readily be expressed as scalar kernels**, e.g. reduction\n",
    "* kernels execute on the GPU where the **Julia runtime isn't available**\n",
    "\n",
    "In particular due to the last point, kernel code has limitations\n",
    "  * no GC\n",
    "  * no `print` etc. (‚Üí `@cuprint`)\n",
    "  * code must be fully type inferred (no dynamic dispatch allowed)\n",
    "  * no `try ... catch ... end`\n",
    "  * ...\n",
    "\n",
    "**You can't just write arbitrary Julia code in kernels.** Fortunately though, many things just work and can get you far (see e.g. exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Hardware limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025) # one more element than before\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA programming model\n",
    "\n",
    "<!-- <img src=\"./imgs/CUDA_programming_model.png\" width=1024> -->\n",
    "<img src=\"imgs/cuda_prog_model.svg\" width=1024>\n",
    "\n",
    "Conceptual mapping:\n",
    "\n",
    "* **Grid** of blocks ‚Üí entire GPU\n",
    "* **Blocks** of threads ‚Üí SMs\n",
    "* **Threads** ‚Üí CUDA cores\n",
    "\n",
    "**Note**: up to three dimensions, $(x, y, z)$, can be used to organize the thread blocks and threads in each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x # global thread index\n",
    "    if i <= length(x) # make sure that we're inbounds (c.f. \"loop\" iteration range)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**execution configuration** for a CUDA kernel:\n",
    "* `threads`: number of threads in each block\n",
    "* `blocks`: number of blocks in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does our CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.rand(1024*1024);\n",
    "\n",
    "function add_one_broadcasting(x)\n",
    "    CUDA.@sync x .+ 1\n",
    "end\n",
    "\n",
    "# same CUDA kernel, but with different execution configurations\n",
    "function add_one_kernel_1024_1k(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_256_4k(x)\n",
    "    CUDA.@sync @cuda threads=256 blocks=4*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_64_16k(x)\n",
    "    CUDA.@sync @cuda threads=64 blocks=16*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "@btime add_one_broadcasting(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_1024_1k(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_256_4k(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_64_16k(x) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a CUDA kernel is affected by the execution configuration (because of the runtime scheduling of thread blocks and threads).\n",
    "\n",
    "How to obtain a good execution configuration for a CUDA kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying kernel launches: [Occupancy API](https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/)\n",
    "\n",
    "Hardcoded execution configuration is not a good idea. A few reasons:\n",
    "\n",
    "* In reality, the actual maximal number of threads can depend on kernel details, like how many resources the kernel is using.\n",
    "* You might want to support different GPUs with different hardware limitations.\n",
    "\n",
    "**Occupancy** measures the ratio of the number of active *warps* per SM to the maximum number of possible warps per SM.\n",
    "\n",
    "*warp*: a group of 32 parallel threads executing the same instructions.\n",
    "\n",
    "* Low occupancy usually implies low performance (because of underutilized hardware resources).\n",
    "* High occupancy, however, may not guarantee the best performance.\n",
    "\n",
    "The occupancy API is an automatic tool that can be used to obtain *reasonably good* execution configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = @cuda launch=false cuda_kernel_blocks!(x) # don't launch the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = CUDA.launch_configuration(kernel.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number `blocks` indicates how many blocks we would need to fully occupy the GPU. For a given input `x`, we might need fewer or more blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threads = min(length(x), config.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blocks = cld(length(x), threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the kernel with the dynamic launch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel(x; threads, blocks) # calling `kernel` like a regular function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime kernel(x; threads=$threads, blocks=$blocks) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `@code_*` for CPU there are `@device_code_*` macros for GPU. However, the GPU pendant for `@code_native` is `@device_code_ptx`.\n",
    "\n",
    "**PTX**: a low-level **p**arallel **t**hread e**x**ecution virtual machine and instruction set architecture used in NVIDIA CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_warntype @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_llvm debuginfo=:none @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_ptx @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs in one compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Note: You can't run this part because you only have a single device in this session.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **4x NVIDIA A100 GPUs** in a regular Noctua 2 GPU node:\n",
    "\n",
    "<!-- <img src=\"./imgs/Noctua2_GPU_node.png\" width=320px> -->\n",
    "<img src=\"imgs/Noctua2_GPU_node.svg\" width=320px>\n",
    "\n",
    "Since each Julia task gets its own local CUDA execution environment, it is easy to utilize multi-GPUs in one compute node to perform computations in parallel by launching multiple Julia tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will use `Threads.@spawn` (since multi-threading support is a rather recent addition to CUDA.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gpu_computation(A, B, C)\n",
    "    for i in 1:512\n",
    "        C = A * B\n",
    "        A = B * C\n",
    "        B = C * A\n",
    "    end\n",
    "    sin.(B)\n",
    "    return B\n",
    "end\n",
    "\n",
    "function multi_gpu()\n",
    "    n = 4096\n",
    "    @sync begin\n",
    "        # Julia task for the 1st GPU\n",
    "        @spawn begin\n",
    "            device!(0) # first GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 1: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 1: done\")\n",
    "        end\n",
    "        # Julia task for the 2nd GPU\n",
    "        @spawn begin\n",
    "            device!(1) # second GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 2: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 2: done\")\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call CUDA.memory_status() for all GPUs\n",
    "for dev in devices()\n",
    "    device!(dev)\n",
    "    println()\n",
    "    CUDA.memory_status()\n",
    "end\n",
    "device!(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device!(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "\n",
    "@btime CUDA.@sync A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"allocations\" here means CPU allocations. For GPU allocations you can e.g. use `CUDA.@time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@time A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated profiler: `CUDA.@profile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile A .* B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile trace=true A .* B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External profiler: [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://cuda.juliagpu.org/stable/development/profiling/#External-profilers\n",
    "\n",
    "**Command**: `CUDA.@profile external=true`\n",
    "\n",
    "Use [**NVTX.jl**](https://github.com/JuliaGPU/NVTX.jl) to annotate (i.e. label and colorize) code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/nsight_systems.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Three ways to SAXPY on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAXPY** = **S**ingle precision **A** times **X** **P**lus **Y**\n",
    "\n",
    "‚Üí exercise **saxpy_gpu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/a100_saxpy_results.png\" width=1024px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core messages of this notebook\n",
    "\n",
    "* GPU architecture is optimal for **high throughput computations using millions of threads**.\n",
    "* For good performance the communication between host and GPU device(s) must be minimized.\n",
    "* CUDA.jl enables to program NVIDIA GPUs in Julia\n",
    "    - high-level array abstraction with `CuArray`\n",
    "        * convenient for some types of computation\n",
    "        * reasonably good performance is achievable\n",
    "    - writing custom CUDA kernels\n",
    "        * flexible for general computations\n",
    "        * **CUDA programming model**\n",
    "            - **Grid of blocks** ‚Üí entire GPU\n",
    "            - **Blocks of threads** ‚Üí SMs\n",
    "            - **Threads** ‚Üí CUDA cores\n",
    "        * the performance is influenced by **the execution configuration**, e.g. number of thread blocks and number of threads per block\n",
    "        * use Occupancy API to obtain an optimal execution configuration for a CUDA kernel\n",
    "    - using the vendor GPU libraries, e.g. cuBLAS, if possible\n",
    "* Multi-GPUs in one compute node can be utilized in parallel with multiple Julia tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.10.0",
   "language": "julia",
   "name": "julia-_8-threads_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
